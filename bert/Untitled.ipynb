{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38905177",
   "metadata": {},
   "source": [
    "### ENV Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63ca1e1e-ad46-4f65-8435-1e98d3808919",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!module load conda\n",
    "!conda activate myenv\n",
    "!module load cudnn/9.1.0\n",
    "!module load nccl/2.21.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0990db9d",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66fc7a41-580c-47e5-8261-aa1972211f37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('IMDB Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a668afa-aa75-4e72-a6b3-3011ac21244d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning:\n",
      "A wonderful little production. <br /><br />The filming technique is very\n",
      "\n",
      "After cleaning:\n",
      "A wonderful little production. The filming technique is very unassuming-\n"
     ]
    }
   ],
   "source": [
    "# Remove the break tags (<br />)\n",
    "df['review_cleaned'] = df['review'].apply(lambda x: x.replace('<br />', ''))\n",
    "\n",
    "# Remove unnecessary whitespace\n",
    "df['review_cleaned'] = df['review_cleaned'].replace('\\s+', ' ', regex=True)\n",
    "\n",
    "# Compare 72 characters of the second review before and after cleaning\n",
    "print('Before cleaning:')\n",
    "print(df.iloc[1]['review'][0:72])\n",
    "\n",
    "print('\\nAfter cleaning:')\n",
    "print(df.iloc[1]['review_cleaned'][0:72])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45439d9f-1fa2-423b-a8f8-480b2899c1ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_cleaned</th>\n",
       "      <th>sentiment_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>A wonderful little production. The filming tec...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive   \n",
       "1  A wonderful little production. <br /><br />The...  positive   \n",
       "2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3  Basically there's a family where a little boy ...  negative   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "\n",
       "                                      review_cleaned  sentiment_encoded  \n",
       "0  One of the other reviewers has mentioned that ...                  1  \n",
       "1  A wonderful little production. The filming tec...                  1  \n",
       "2  I thought this was a wonderful way to spend ti...                  1  \n",
       "3  Basically there's a family where a little boy ...                  0  \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...                  1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment_encoded'] = df['sentiment'].apply(lambda x: 0 if x == 'negative' else 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b8f7af",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7675acf2-56e2-4163-8534-fee72c2e0c05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install 'transformers[torch]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23424b1c-44e9-4c1d-ab2b-6ceb58234fec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/y/ycryu/.conda/envs/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e79853ac-9485-4615-939d-02e19ed18337",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [ 101 1045 4669 2023 3185  102]\n",
      "Tokens   : ['[CLS]', 'i', 'liked', 'this', 'movie', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Just for test - return tensor pt/tf/np for each type\n",
    "sample_sentence = 'I liked this movie'\n",
    "token_ids = tokenizer.encode(sample_sentence, return_tensors='np')[0]\n",
    "print(f'Token IDs: {token_ids}')\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "print(f'Tokens   : {tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbadecfd-e351-4109-b674-b0fabb3a2f6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7eb5372-7bab-4bdd-a23b-9bbf813522b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2028,  1997,  1996,  2060, 15814,  2038,  3855,  2008,  2044,\n",
      "          3666,  2074,  1015, 11472,  2792,  2017,  1005,  2222,  2022, 13322,\n",
      "          1012,  2027,  2024,  2157,  1010,  2004,  2023,  2003,  3599,  2054,\n",
      "          3047,  2007,  2033,  1012,  1996,  2034,  2518,  2008,  4930,  2033,\n",
      "          2055, 11472,  2001,  2049, 24083,  1998,  4895, 10258,  2378,  8450,\n",
      "          5019,  1997,  4808,  1010,  2029,  2275,  1999,  2157,  2013,  1996,\n",
      "          2773,  2175,  1012,  3404,  2033,  1010,  2023,  2003,  2025,  1037,\n",
      "          2265,  2005,  1996,  8143, 18627,  2030,  5199,  3593,  1012,  2023,\n",
      "          2265,  8005,  2053, 17957,  2007, 12362,  2000,  5850,  1010,  3348,\n",
      "          2030,  4808,  1012,  2049,  2003, 13076,  1010,  1999,  1996,  4438,\n",
      "          2224,  1997,  1996,  2773,  1012,  2009,  2003,  2170, 11472,  2004,\n",
      "          2008,  2003,  1996,  8367,  2445,  2000,  1996, 17411,  4555,  3036,\n",
      "          2110,  7279,  4221, 12380,  2854,  1012,  2009,  7679,  3701,  2006,\n",
      "         14110,  2103,  1010,  2019,  6388,  2930,  1997,  1996,  3827,  2073,\n",
      "          2035,  1996,  4442,  2031,  3221, 21430,  1998,  2227, 20546,  2015,\n",
      "          1010,  2061,  9394,  2003,  2025,  2152,  2006,  1996, 11376,  1012,\n",
      "          7861,  2103,  2003,  2188,  2000,  2116,  1012,  1012, 26030,  2015,\n",
      "          1010,  7486,  1010, 18542, 10230,  1010,  7402,  2015,  1010,  8135,\n",
      "          1010, 16773,  1010,  3493,  1998,  2062,  1012,  1012,  1012,  1012,\n",
      "          2061,  8040, 16093, 28331,  1010,  2331, 14020,  1010, 26489,  6292,\n",
      "         24069,  1998, 22824, 10540,  2024,  2196,  2521,  2185,  1012,  1045,\n",
      "          2052,  2360,  1996,  2364,  5574,  1997,  1996,  2265,  2003,  2349,\n",
      "          2000,  1996,  2755,  2008,  2009,  3632,  2073,  2060,  3065,  2876,\n",
      "          1005,  1056,  8108,  1012,  5293,  3492,  4620,  4993,  2005,  7731,\n",
      "          9501,  1010,  5293, 11084,  1010,  5293,  7472,  1012,  1012,  1012,\n",
      "         11472,  2987,  1005,  1056,  6752,  2105,  1012,  1996,  2034,  2792,\n",
      "          1045,  2412,  2387,  4930,  2033,  2004,  2061, 11808,  2009,  2001,\n",
      "         16524,  1010,  1045,  2481,  1005,  1056,  2360,  1045,  2001,  3201,\n",
      "          2005,  2009,  1010,  2021,  2004,  1045,  3427,  2062,  1010,  1045,\n",
      "          2764,  1037,  5510,  2005, 11472,  1010,  1998,  2288, 17730,  2000,\n",
      "          1996,  2152,  3798,  1997,  8425,  4808,  1012,  2025,  2074,  4808,\n",
      "          1010,  2021, 21321,  1006, 15274,  4932,  2040,  1005,  2222,  2022,\n",
      "          2853,  2041,  2005,  1037, 15519,  1010, 13187,  2040,  1005,  2222,\n",
      "          3102,  2006,  2344,  1998,  2131,  2185,  2007,  2009,  1010,  2092,\n",
      "          5450,  2098,  1010,  2690,  2465, 13187,  2108,  2357,  2046,  3827,\n",
      "          7743,  2229,  2349,  2000,  2037,  3768,  1997,  2395,  4813,  2030,\n",
      "          3827,  3325,  1007,  3666, 11472,  1010,  2017,  2089,  2468,  6625,\n",
      "          2007,  2054,  2003,  8796, 10523,  1012,  1012,  1012,  1012,  2008,\n",
      "          2015,  2065,  2017,  2064,  2131,  1999,  3543,  2007,  2115,  9904,\n",
      "          2217,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]])\n"
     ]
    }
   ],
   "source": [
    "review = df['review_cleaned'].iloc[0]\n",
    "\n",
    "token_ids = tokenizer.encode(\n",
    "    review,\n",
    "    max_length = 512,\n",
    "    padding = 'max_length',\n",
    "    truncation = True,\n",
    "    return_tensors = 'pt')\n",
    "\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01c99b04-a045-43fa-9b80-518654d19019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Preprocessing data\n",
      "Tokenized data loaded from tokenized_data.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "output_file = 'tokenized_data.pkl'\n",
    "def tokenize_data(df):\n",
    "    token_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    for i, review in enumerate(df['review_cleaned']):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"{i} data processed\")\n",
    "        \n",
    "        batch_encoder = tokenizer.encode_plus(\n",
    "            review,\n",
    "            max_length = 512,\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            return_tensors = 'pt'\n",
    "        )\n",
    "\n",
    "        token_ids.append(batch_encoder['input_ids'])\n",
    "        attention_masks.append(batch_encoder['attention_mask'])\n",
    "\n",
    "    token_ids = torch.cat(token_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    return token_ids, attention_masks\n",
    "\n",
    "def save_tokenized_data(token_ids, attention_masks, output_file):\n",
    "    data = {'token_ids' : token_ids, 'attention_masks' : attention_masks}\n",
    "\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"Tokenized data saved as {output_file}\")\n",
    "\n",
    "def load_tokenized_data(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    print(f\"Tokenized data loaded from {file_path}\")\n",
    "    return data['token_ids'], data['attention_masks']\n",
    "\n",
    "\n",
    "if not os.path.exists(output_file):\n",
    "    print(\"Preprocessing started\")\n",
    "    token_ids, attention_masks = tokenize_data(df)\n",
    "    save_tokenized_data(token_ids, attention_masks, output_file)\n",
    "else:\n",
    "    print(\"Loaded Preprocessing data\")\n",
    "    token_ids, attention_masks = load_tokenized_data(output_file)\n",
    "\n",
    "len(token_ids)\n",
    "len(attention_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5c238c",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c5447ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if GPU is available for faster training time\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a6306c7-470a-43bb-96b8-7113757e7cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "val_size = 0.1\n",
    "\n",
    "# Split the token IDs\n",
    "train_ids, val_ids = train_test_split(\n",
    "                        token_ids,\n",
    "                        test_size=val_size,\n",
    "                        shuffle=False)\n",
    "\n",
    "# Split the attention masks\n",
    "train_masks, val_masks = train_test_split(\n",
    "                            attention_masks,\n",
    "                            test_size=val_size,\n",
    "                            shuffle=False)\n",
    "\n",
    "# Split the labels\n",
    "labels = torch.tensor(df['sentiment_encoded'].values)\n",
    "train_labels, val_labels = train_test_split(\n",
    "                                labels,\n",
    "                                test_size=val_size,\n",
    "                                shuffle=False)\n",
    "\n",
    "# Create the DataLoaders\n",
    "train_ids = train_ids.to(device)\n",
    "train_masks = train_masks.to(device)\n",
    "train_labels = train_labels.to(device)\n",
    "val_ids = val_ids.to(device)\n",
    "val_masks = val_masks.to(device)\n",
    "val_labels = val_labels.to(device)\n",
    "\n",
    "train_data = TensorDataset(train_ids, train_masks, train_labels)\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=16)\n",
    "val_data = TensorDataset(val_ids, val_masks, val_labels)\n",
    "test_dataloader = DataLoader(val_data, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9167bb12",
   "metadata": {},
   "source": [
    "### Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c026920a-7eb3-41bb-abe6-8324d7ebb3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6459e8df",
   "metadata": {},
   "source": [
    "### Instantiate Optim, loss fn, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "31e304cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "num_training_steps = EPOCHS * len(train_dataloader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=num_training_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2034d538",
   "metadata": {},
   "source": [
    "### Training\n",
    "It seems that it works well, but i think i have to make job and upload to GPU server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "59d0c374",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|          | 30/2813 [00:11<17:59,  2.58it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 20\u001b[0m\n\u001b[1;32m     12\u001b[0m loss, logits \u001b[38;5;241m=\u001b[39m model(\n\u001b[1;32m     13\u001b[0m     batch_token_ids,\n\u001b[1;32m     14\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mbatch_attention_mask,\n\u001b[1;32m     16\u001b[0m     labels\u001b[38;5;241m=\u001b[39mbatch_labels,\n\u001b[1;32m     17\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m training_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 20\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(0, EPOCHS):\n",
    "\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}'):\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch_token_ids = batch[0].to(device)\n",
    "        batch_attention_mask = batch[1].to(device)\n",
    "        batch_labels = batch[2].to(device)\n",
    "\n",
    "        loss, logits = model(\n",
    "            batch_token_ids,\n",
    "            token_type_ids = None,\n",
    "            attention_mask=batch_attention_mask,\n",
    "            labels=batch_labels,\n",
    "            return_dict=False)\n",
    "\n",
    "        training_loss += loss.item()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    average_train_loss = training_loss / len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1b812837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:38<00:00,  8.07it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def calculate_accuracy(preds, labels):\n",
    "    \"\"\" Calculate the accuracy of model predictions against true labels.\n",
    "\n",
    "    Parameters:\n",
    "        preds (np.array): The predicted label from the model\n",
    "        labels (np.array): The true label\n",
    "\n",
    "    Returns:\n",
    "        accuracy (float): The accuracy as a percentage of the correct\n",
    "            predictions.\n",
    "    \"\"\"\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    accuracy = np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def calculate_accuracy_gpu(preds, labels):\n",
    "    pred_flat = torch.argmax(preds, dim=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    accuracy = torch.sum(pred_flat == labels_flat).item() / len(labels_flat)\n",
    "    return accuracy\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "val_loss = 0\n",
    "val_accuracy = 0\n",
    "\n",
    "for batch in tqdm(test_dataloader):\n",
    "\n",
    "    batch_token_ids = batch[0].to(device)\n",
    "    batch_attention_mask = batch[1].to(device)\n",
    "    batch_labels = batch[2].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        (loss, logits) = model(\n",
    "            batch_token_ids,\n",
    "            attention_mask = batch_attention_mask,\n",
    "            labels = batch_labels,\n",
    "            token_type_ids = None,\n",
    "            return_dict=False)\n",
    "\n",
    "    # For CPU function\n",
    "    # logits = logits.detach().cpu().numpy()\n",
    "    # label_ids = batch_labels.to('cpu').numpy()\n",
    "    # val_loss += loss.item()\n",
    "    # val_accuracy += calculate_accuracy(logits, label_ids)\n",
    "\n",
    "    val_loss += loss.item()\n",
    "    val_accuracy += calculate_accuracy_gpu(logits, batch_labels)\n",
    "\n",
    "average_val_accuracy = val_accuracy / len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8d5cc568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4936102236421725"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1653b112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
